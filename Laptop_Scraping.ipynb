{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b955bf0-6807-4f66-9085-d5951351f3f1",
   "metadata": {},
   "source": [
    "# PROJET DE PYTHON\n",
    "# THEME : ANALYSE DE LA SATISFACTION DES UTILISATEURS DE LAPTOPS : CAS DES LAPTOPS DELL\n",
    "# Auteurs:\n",
    "* MECHIDE SERINE\n",
    "* METHAFE KUITE SORELLE LOVELINE\n",
    "* NKAMENI DANIEL\n",
    "* POUANI EMAPI HORNELLA JOÊLLE\n",
    "\n",
    "# SCRAPING DES DONNEES SUR LES LAPTOPS DELL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1212da4-ae8e-4f20-ae6f-8914a7b04046",
   "metadata": {},
   "source": [
    "Le but de cette partie est de scraper tous les informations (caractéristiques et commentaires) sur les machines proposées sur le site officiel de Dell France au 10 Dcembre 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d2bdf3-9bf2-4a10-a521-fca5e761350b",
   "metadata": {},
   "source": [
    "## SCRAPING DES COMMENTAIRES SUR LES LAPTOPS DELL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3e910-2a3a-4771-b785-b5f465fdcd06",
   "metadata": {},
   "source": [
    "Pour scraper les commentaires de chaque machine, il incombe d'ouvrir individuellement les pages de chacune d'elles. Les liens vers la page détaillée de chaque machine sont obtenu à l'aide d'un premier craping. Les pages détaillées des machines sont dynamiques et chargées avec Java Script. Les méthodes de scraping basic vu en cours ne suffiront donc pas dans ce cas. Nous utilisons donc dans la suite le package Selenium adapt au scraping des pages dynamique.\n",
    "\n",
    "Le pilote Web utilisé est Chrome. l'étape suivante est donc de configurer ce pilote. On définit également un temps d'attente necessaire au chargement de chaque page. Nous avons défini un temps d'attente de 5 secondes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92b9765-85b3-41e4-836c-08125679f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des packages\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "import requests  \n",
    "import re\n",
    "import time  \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from joblib import Parallel, delayed \n",
    "import itertools\n",
    "import urllib\n",
    "import bs4\n",
    "import pandas as pd\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b112d-8f7c-48ed-a8c4-3e5f6c2f681b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Installer chrome driver\n",
    "driver_path = 'C:/Users/nkame/anaconda3/chromedriver.exe'\n",
    "opt = webdriver.ChromeOptions()\n",
    "opt.add_experimental_option('w3c', False)\n",
    "driver = webdriver.Chrome(executable_path=driver_path,options=opt)\n",
    "\n",
    "\n",
    "##Initialiser le driver de Selenium\n",
    "def setupDriver(url, waiting_time = 5):\n",
    "\tdriver = webdriver.Chrome(options=opt)\n",
    "\tdriver.get(url)\n",
    "\ttime.sleep(waiting_time) \n",
    "\treturn driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635be3e-0a42-4081-8e82-520131f14710",
   "metadata": {},
   "source": [
    "La page de chaque machine affiche les premiers commentaires de cette macine en bas de page. Pour les machines dontles commentaires ne tiennent pas sur une page, il faut retrouver l'url de la page suivante et l'utiliser pour acceder aux commentaires suivants. \n",
    "\n",
    "Attention: Pour la première page de commentaires comme les page suivante, il est important d'entrer exactement l'url de la section commentaire sinon l'extraction n'aura pas lieu.\n",
    "\n",
    "Le code suivant permet de charger chaque page detaillée et recupérer le lien de la page des commentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c61f3-441c-4b2e-a7f9-4879a7259d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Charger la page de chaque machine\n",
    "def getJSpage(url):\n",
    "\tdriver = setupDriver(url)\n",
    "\thtml = driver.page_source\n",
    "\tdriver.close()\n",
    "\treturn BeautifulSoup(html, features=\"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb310a-be35-4c1d-ac39-bf88764c0e24",
   "metadata": {},
   "source": [
    "Une fois la page des commentaires ouverte, il incombe de déterminer la classe qui sera utilisée pour l'extraction. l'inspection de la page revèle qu'il s'agit d'un objet \"div\" de classe \"bv-content-summary-body-text\". \n",
    "\n",
    "Dans cette classe, sont présents les commentaires des clients ainsi que ceux de Dell. Nous allons par la suite crire une fonction qui suprime les commentaires de Dell.\n",
    "\n",
    "Rappelonsque les commentaires de certaines machines peuvent se retrouver sur plus d'une page et doivent être pris en compte. La fonction \"getNextPage\" dans le code suivant permet donc de recuperer l'url de la page suivante dans la page actuelle afin de pouvoir s'y rendre une fois le scraping sur la page actuelle terminé.\n",
    "\n",
    "La fonction de recupération des commentaires d'une machine est conçue de façon à fonctionner de façon totalement autonome et adaptative en fonction du nombre de pages de commentaires disponibles pour cette machine. En effet, la fonction recupère les commentaires sur une page, vérifie s'il y a une page suivante et si oui passe à celle-ci et recupère les commentaires aussi. Elle s'arrête quand toutes les pages de commentaires ont t visit et scrapé. La structure du site de Dell est assez bizarre mais nous avons réussi à pouvoir implementer cette fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b3c37-6eb0-45ac-98f7-6ae418146049",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Recuperer l'ensemble des commentaires\n",
    "\n",
    "# Fonction qui recupère les commentaires d'une page\n",
    "def getPageComments(soupage):\n",
    "    cpage = soupage.findAll('div', {'class': 'bv-content-summary-body-text'})\n",
    "    pagecom = [x.text.strip() for x in cpage]\n",
    "    return pagecom\n",
    "\n",
    "# Fonction qui recupère l'URL de la prochaine page de commentaires\n",
    "def getNextPage(soupage):\n",
    "    nextPage = soupage.findAll('a', {'class': 'bv-content-btn bv-content-btn-pages bv-content-btn-pages-last bv-focusable bv-content-btn-pages-active'})\n",
    "    return nextPage\n",
    "\n",
    "# Fonction de recupration des commentaires pour une machine\n",
    "def getcomments(NCom,Machinelink):\n",
    "    PageM = getJSpage(Machinelink)\n",
    "    Pagecom = []\n",
    "    Pagecom = Pagecom + getPageComments(PageM)\n",
    "    nextPage = getNextPage(PageM)\n",
    "    \n",
    "    # Vérification de l'existence d'une page suivnate\n",
    "    while(len(nextPage) != 0):\n",
    "        PageM2 = getJSpage(nextPage[0][\"href\"])\n",
    "        Pagecom = Pagecom + getPageComments(PageM2)\n",
    "        nextPage = getNextPage(PageM2)\n",
    "    \n",
    "    # Création des indices (numéro de commande unique pour chaque machine) et de la fameuse base de données\n",
    "    MIndex = [NCom]*len(Pagecom)\n",
    "    comments = pd.DataFrame()\n",
    "    comments[\"Comments\"] = Pagecom\n",
    "    comments.index = MIndex\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b5d53-2f49-4a97-9061-139f190b357b",
   "metadata": {},
   "source": [
    "Une fois les commentaire récupérés, il faut supprimier les commentaires de Dell. Le souci est que tous les commentaires (clients et Dell) sont dans les mêmes balises et classes. Il est clair qu'utiliser les balises ou les classes pour les séparer allait être très compliqué. Nous avons donc analysé les commentaires de Dell et nous avons remarqu qu'ils avaient tous une signature éléctronique contenant \"@Dell\" ou \"@ Dell\".\n",
    "\n",
    "Ceci nous a données l'idée de scraper tous les commentaires et ensuite de supprimer tous ceux contenant @Dell ou @ Dell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01456f-3732-4ae6-ac19-7a8b4edc4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des commentaires de DELL\n",
    "def DelDellComments(Comments):\n",
    "    CleanComments = Comments[[(('@Dell' not in str(X))&('@ Dell' not in str(X))) for X in Comments[\"Comments\"]]]\n",
    "    return CleanComments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b4141-34f6-4fbc-bd74-f7e3e4f3aa17",
   "metadata": {},
   "source": [
    "Avec toutes ces fonctions, nous pouvons à prsent écrire la fonction de récupérations de tous les commentaires des machines disponibles sur le site de Dell. Cette fonction prend en entrée un Data Frame conténant les codes de commande (identificateurs unique de chaque machine) et les url de toutes les machines et renvoie un Data Frame de commenatires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9687d8c-3d4c-4586-8f7b-1e0a0fc84a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la fonction de recupération de tous les commentaires\n",
    "def getAllComments(Machines):\n",
    "    AllComments = pd.DataFrame()\n",
    "    for i in range(len(Machines)):\n",
    "        Com = getcomments(Machines.iloc[i,0],Machines.iloc[i,1])\n",
    "        AllComments = pd.concat([AllComments,Com],axis=0)\n",
    "    AllComments = DelDellComments(AllComments)\n",
    "    return AllComments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e8b67-1beb-420a-a42e-9b90f1bc12ac",
   "metadata": {},
   "source": [
    "Le bloc de code suivant permet de récupérer les url de toutes les machines à l'aide d'un scraping classique sur le site de de Dell. Ceci permettra de pouvoir construire le Data Frame néccessaire à la récupération des commentaires comme décrit plus haut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc832f64-8df3-4417-a2c6-12a058e0bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catégories de machines et nombre de pages sur le site de Dell\n",
    "\n",
    "list_ref_categ = [[\"https://www.dell.com/fr-fr/shop/ordinateurs-portables-dell/sr/laptops/inspiron-laptops\",6] , [\"https://www.dell.com/fr-fr/shop/ordinateurs-portables-dell/sr/laptops/xps-laptops\",2], [\"https://www.dell.com/fr-fr/shop/ordinateurs-portables-dell/sr/laptops/g-series\",2] , [\"https://www.dell.com/fr-fr/shop/ordinateurs-portables-dell/sr/laptops/alienware-laptops\",3 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06b957-2e46-45c3-80c7-f26aa9e71e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de récupérateion des codes des dmachines et les url de toutes les machines\n",
    "\n",
    "def url (url):\n",
    "  el=[]\n",
    "  codess=[]\n",
    "  for i  in range (1,int(url[1])+1) :\n",
    "    urlel= url[0] + \"?page=\" + str(i) \n",
    "    req = request.Request(urlel,headers={'User-Agent': 'Mozilla/5.0'})    \n",
    "    html = request.urlopen(req).read()\n",
    "    pagefille= bs4.BeautifulSoup(html,\"lxml\")\n",
    "    elementss = pagefille.find_all('div', class_ = \"ps-image ps-product-image\")  # elementss est une liste dont chaque élément contient une balise de type \"a\" contenant l'url de la machine considérée\n",
    "    code = pagefille.find_all('div', class_ = \"ps-compare\") # code est une liste dont chaque élément contient une balise de type \"label\" contenant le code de commande d'une machine\n",
    "    elementss_ref= [element.a['href'] for element in elementss]\n",
    "    el = el + elementss_ref\n",
    "    codes=[cod.label['oc'] for cod in code ]\n",
    "    codess = codess + codes\n",
    "  return el, codess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a4527-2cb5-4f1f-95ad-df270aca054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération proprement dite des codes machines et url\n",
    "\n",
    "elements=[]\n",
    "codes=[]\n",
    "\n",
    "for i in range(len(list_ref_categ)):\n",
    "  el,codess = url(list_ref_categ[i])\n",
    "  elements = elements + el\n",
    "  codes = codes + codess\n",
    "\n",
    "useful_urls = [\"https:\" + str(raw_url) + \"#ratings_section\" for raw_url in elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ad036-ecf7-4ae7-9fc9-cea06dfbc425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du Data Frame contenant toutes les machines\n",
    "\n",
    "All_machines = pd.DataFrame({'code_commande': codes, 'url_ordi':useful_urls}).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb6abf-a4fa-4f9c-b8aa-bd80bd7f70ac",
   "metadata": {},
   "source": [
    "A ce stade, nous pouvons enfin récupérer les commentaires de toutes les machines sur le site de Dell France..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b392e8-775f-4af1-9b5b-882d5c5b5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webscraping des commentaires hahaha\n",
    "# NB: La fonction utilisée ici supprime aussi les commentaire de Dell et garde juste les commentaires clients\n",
    "\n",
    "All_comments = getAllComments(All_machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157458fb-29d7-470d-8cb0-a1daa5a69088",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9bdcce-6e01-4989-a3bf-8b6d00255935",
   "metadata": {},
   "source": [
    "Pour la suite des analyse, nous enrégistrons ces commentaires dans un fichier Excel car le scraping prend environ 1h30 et nous ne pourrons pas exécuter le code à chaque fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e516f-99bc-4927-b4df-9077611c55fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_comments.to_excel(\"C:/Users/nkame/Desktop/ND/COURS ET TD/PYTHON/PROJET/Commentaires.xlsx\", header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af23fa3-6242-469f-8511-97a37f318938",
   "metadata": {},
   "source": [
    "## SCRAPING DES CARACTERISTIQUES SUR LES LAPTOPS DELL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320a7a4-9e12-4c86-bf61-14e063978975",
   "metadata": {},
   "source": [
    "Il y' a sur le site de DELL France, 4 catégories d'ordinateyrs particuliers. pour wedscrapper leur différentes caractéristiques, il sera question pour nous, pour chaque catégorie, d'accéder au lien de chaque machine, à fin de récupérer les caractéristiques.\n",
    "\n",
    "Les pages détaillées des machines sont dynamiques et chargées avec Java Script. Les méthodes de scraping basic vu en cours ne suffiront donc pas dans ce cas. Nous utilisons donc dans la suite le package Selenium adapt au scraping des pages dynamique.\n",
    "\n",
    "Le pilote Web utilisé sera Chrome tout comme dans la partie précédente. Il ne sera donc plus nécéssaire d'importer les bibliothèque. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c8904-8c08-4976-bc78-1257ccd4ad40",
   "metadata": {},
   "source": [
    "Les caractéristiques des ordinateurs dépendent de la catégorie dans laquelle elle se trouve. Donc les fonctions écrites ici dépendront des catégories.\n",
    "\n",
    "Pour commencer, il nous faut les url des machiines de chaque catégorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23b409-e9d1-4ad2-af01-b8daf955c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ref_categ = [ [\"https://www.dell.com/fr-fr/shop/ordinateurs-portables-dell/sr/laptops/inspiron-laptops\",6,'/html/body/main/section[2]/div[1]/div[1]/div[2]/div[2]/span[1]/span','/html/body/main/section[2]/div[2]/div/ul/li'] , [\"https://www.dell.com/fr-fr/shop/ordinateurs-portables-dell/sr/laptops/xps-laptops\",2,'/html/body/main/section[2]/div[1]/div[1]/div[2]/div[2]/span[1]/span','/html/body/main/section[2]/div[2]/div/ul/li'], [\"https://www.dell.com/fr-fr/shop/ordinateurs-portables-dell/sr/laptops/g-series\",2,'//*[@id=\"cf-strike-through-price\"]/div[2]/div','//*[@id=\"m_146\"]/div[2]'] , [\"https://www.dell.com/fr-fr/shop/ordinateurs-portables-dell/sr/laptops/alienware-laptops\",3 ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3084d030-6f91-48ec-b31e-85cb1c0d5ae1",
   "metadata": {},
   "source": [
    "Chaque élément de cette liste est une liste, dont le premier élément est le lien d'une catégorie d'ordinateurs, le second est le nombre de pages web sur lequel s'étend cette catégorie. le troisième et le quatrième élément  sont des \"x-path\", qui nous aiderons dans le scrapping. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282be289-fed5-44cd-aa2d-b18dab516d85",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "1.    **Fonction de récupération des url  ainsi que des numéros de commandes correspondants des ordinateurs présents sur une page web**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2fae8-8276-45ff-a339-7e9ef81c4335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url (urlel):   \n",
    "  driver=webdriver.Chrome('chromedriver', options=options)\n",
    "  driver.get(urlel) \n",
    "  html = driver.page_source\n",
    "  pagefille= bs4.BeautifulSoup(html,\"lxml\")\n",
    "  elementss = pagefille.find_all('div', class_ = \"ps-image ps-product-image\")  # elementss est une liste dont chaque élément contient une balise de type \"a\" contenant l'url de la machine considérée\n",
    "  code = pagefille.find_all('div', class_ = \"ps-compare\") # code est une liste dont chaque élément contient une balise de type \"label\" contenant le code de commande d'une machine\n",
    "  elementss_ref= [[element.a['href']] for element in elementss]   # liste contenant les url des différentes machines de la page web\n",
    "  for i in range (len(code)):\n",
    "    elementss_ref[i].append(code[i].label['oc'])\n",
    "  return elementss_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f91c33-3894-451b-bfa0-1d06abd5b958",
   "metadata": {},
   "source": [
    "Cette fonction prend en entrée le lien d'une catégorie et nous renvoie une liste. Chaque élément de cette liste est une liste de longeur 2, tel que le premier élément correspond à l'url d'une machine et le deuxième au numéro de commande de ladite machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca144c5-8647-47c1-97d3-2399cd1ec9d9",
   "metadata": {},
   "source": [
    "2.   **Fonction de récupération des caractéristiques d'une machines selon sa catégorie.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc27d85-612c-4d60-a4c0-ca7940af74a9",
   "metadata": {},
   "source": [
    "Pour le faire,nous ferrons trois fonctions différentes. Une pour les ordinateurs des catégories inspiron et xps, une pour celles de la catégorie g_series et une autre pour celles de la catégorie aliewner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fcb386-2b90-470f-a58b-958d2524cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des carctéristiques d'un ordinateir de type inspiron ou xps\n",
    "def caract_insp_xps (ref,xpath_prix, xpath):\n",
    "  url_pagefilleel= \"http:\"+ ref[0] + \"#tech-specs-anchor\"\n",
    "  driver=webdriver.Chrome('chromedriver', options=options)\n",
    "  driver.get(url_pagefilleel)    \n",
    "  dict_caractel = dict()   # initialisation du dictionnaire des caractéristiques\n",
    "  caract1= driver.find_elements_by_xpath(xpath_prix) # prix original\n",
    "  caract1el= caract1[0].text\n",
    "  dict_caractel['prix']= caract1el# ajouter le prix original dans le dictionnaire des caractéristiques\n",
    "  xpath_caract= xpath+'/p'\n",
    "  xpath_name= xpath + '/div'\n",
    "  nameel= driver.find_elements_by_xpath(xpath_name)\n",
    "  caractel= driver.find_elements_by_xpath(xpath_caract)\n",
    "  for i in range(len(caractel)):\n",
    "    caracte= caractel[i].text\n",
    "    name= nameel[i].text\n",
    "    dict_caractel[name]= caracte\n",
    "  return ( pd.DataFrame.from_dict(dict_caractel,orient='index',columns=[str(ref[1])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd426b-9583-413b-878f-d1fa8c48978c",
   "metadata": {},
   "source": [
    "Cette fonction, pren d en entrée l'url de l'ordinateur ainsi que le \"x-path\" du prix et des autres caractéristiques. Elle renvoie un data frame dont constitué du numéro de commande de chaque ordinateur ainsi que de ses différentes caractéristiques.  \n",
    "\n",
    "Nous écrivons le même type de fonction pour les deux autres catégories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8fbd9-7f8b-440b-b9cf-c9f1f8d51298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des carctéristiques d'un ordinateir de type g-series\n",
    "def caract_g_series (ref):\n",
    "  url_pagefilleel= \"http:\"+ ref[0] + \"#tech-specs-anchor\"\n",
    "  driver=webdriver.Chrome('chromedriver', options=options)\n",
    "  driver.get(url_pagefilleel) \n",
    "  html = driver.page_source\n",
    "  driver.close()\n",
    "  pagefilleel= bs4.BeautifulSoup(html, features=\"lxml\")\n",
    "  dict_caractel = dict()  # initialisation du dictionnaire des caractéristiques\n",
    "  prix= pagefilleel.find('div', class_ = \"cf-dell-price\").text.strip() #prix\n",
    "  dict_caractel['prix']= prix # ajout du prix dans le dictionnaire\n",
    "  name_caract= pagefilleel.find_all('h2', class_ = \"ux-module-title\")\n",
    "  processeur= pagefilleel.find('div', class_ = \"ux-readonly-title\").text.strip()# processeur\n",
    "  dict_caractel[name_caract[0].text.strip()]= processeur\n",
    "  sys= pagefilleel.find('div', class_ = \"ux-cell-title\").text.strip() # système d'exploitation\n",
    "  dict_caractel[name_caract[1].text.strip()]= sys\n",
    "  caract= pagefilleel.find_all('div', class_ = \"ux-readonly-title \")\n",
    "  for i in  range(1,6):\n",
    "    dict_caractel[name_caract[i+1].text.strip()]= caract[i].text.strip()  \n",
    "  dict_caractel[name_caract[6].text.strip()]= pagefilleel.find_all('div', class_ = \"ux-cell-title\")[3].text.strip()\n",
    "  for i in  range(6, len(caract)):\n",
    "    dict_caractel[name_caract[i+2].text.strip()]= caract[i].text.strip()  \n",
    "  dict_caractel[name_caract[len(caract)+1].text.strip()]= pagefilleel.find_all('div', class_ = \"ux-cell-title\")[5].text.strip()\n",
    "  dict_caractel[name_caract[len(caract)+2].text.strip()]= pagefilleel.find_all('div', class_ = \"ux-cell-title\")[6].text.strip()\n",
    "  dict_caractel[name_caract[len(caract)+3].text.strip()]= pagefilleel.find_all('div', class_ = \"ux-cell-title\")[7].text.strip()\n",
    "  dict_caractel[name_caract[len(caract)+4].text.strip()]= pagefilleel.find_all('div', class_ = \"ux-cell-title\")[8].text.strip()\n",
    "  dict_caractel[name_caract[len(caract)+5].text.strip()]= pagefilleel.find_all('div', class_ = \"ux-cell-title\")[9].text.strip()\n",
    "  dict_caractel[name_caract[len(caract)+6].text.strip()]= pagefilleel.find_all('div', class_ = \"ux-cell-title\")[10].text.strip()\n",
    "  return( pd.DataFrame.from_dict(dict_caractel,orient='index',columns=[str(ref[1])]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef19cb-dcae-4ed4-b4a3-549f9c558040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des carctéristiques d'un ordinateir de type aliewner\n",
    "def caract_aliewener (ref):\n",
    "  url_pagefilleel= \"http:\"+ ref[0] + \"#tech-specs-anchor\"\n",
    "  driver=webdriver.Chrome('chromedriver', options=options)\n",
    "  driver.get(url_pagefilleel) \n",
    "  html = driver.page_source\n",
    "  driver.close()\n",
    "  pagefilleel= bs4.BeautifulSoup(html, features=\"lxml\")\n",
    "  dict_caractel = dict()  # initialisation du dictionnaire des caractéristiques\n",
    "  prix= pagefilleel.find('div', class_ = \"cf-dell-price\").text.strip() #prix\n",
    "  dict_caractel['prix']= prix # ajout du prix dans le dictionnaire\n",
    "  name_caract= pagefilleel.find_all('h2', class_ = \"ux-module-title\")\n",
    "  caract= pagefilleel.find_all('div', class_ = \"ux-readonly-title \")\n",
    "  for i in  range(len(caract)):\n",
    "    dict_caractel[name_caract[i].text.strip()]= caract[i].text.strip() \n",
    "  #for i in range(3,len(caract)):\n",
    "    #dict_caractel[name_caract[i].text.strip()]= caract[i].text.strip()\n",
    "  dict_caractel[name_caract[len(caract)+1].text.strip()]= pagefilleel.find_all('div', class_ = \"ux-cell-title\")[1].text.strip()\n",
    "  dict_caractel[name_caract[len(caract)+2].text.strip()]= pagefilleel.find_all('div', class_ = \"ux-cell-title\")[2].text.strip()\n",
    "  dict_caractel[name_caract[len(caract)+3].text.strip()]= pagefilleel.find_all('div', class_ = \"ux-cell-title\")[3].text.strip()\n",
    "  dict_caractel[name_caract[len(caract)+4].text.strip()]= pagefilleel.find_all('div', class_ = \"ux-cell-title\")[4].text.strip()\n",
    "  return( pd.DataFrame.from_dict(dict_caractel,orient='index',columns=[str(ref[1])]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e93ff-7cd8-4cdd-bc81-a22945a51ac1",
   "metadata": {},
   "source": [
    "Ces deux fonctions prennent en paramètre uniquement l'url d'un ordinateur. La méthode avec le \"x-path\" ne fonctionne pas ici , car, chaque caractéristique a son \"x-path\". Il est donc difficile de généraliser.\n",
    "\n",
    "Il est question dans la suite de récupérer les caractéristiques de tous les ordinateurs présents sur une page web, selon la catégorie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ff915-e791-4614-9324-5575c50dc224",
   "metadata": {},
   "source": [
    "3.    **Fonction de récupération des caractéristiques des machines sur une page web selon sa catégorie.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2aa7c4-f996-477e-a8da-6e2322f1bab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# récupération des carctéristiques  des ordinateurs de type inspiron ou xps d'une page web donnée.\n",
    "def base_inter_insp_xps ( url_page,xpath_prix,xpath_caract):\n",
    "  elementss_ref= url(url_page)   # renvoie la liste l'url des machines sur url_page et les codes de commandes associés  \n",
    "  data1= pd.DataFrame()      # initialisation du dataframe qui comportera  les machine d'une pageweb ainsi que leurs caractéristiques\n",
    "  for i in range (len(elementss_ref)):\n",
    "    data_inter= caract_insp_xps(elementss_ref[i],xpath_prix,xpath_caract)  # dataframe d'un ordi avec ses caractéristiques\n",
    "    data1= pd.concat([data1, data_inter],axis=1,sort=True)     \n",
    "  return data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1890bba-be0e-4280-92e5-7449ec1640b0",
   "metadata": {},
   "source": [
    "Cette fonction prend en  entrée l'url de la page web et renvoie un data frame, constitué des numéros de commandes et descaractéristiques des ordinateurs présentes sur la page web correspondante.\n",
    "\n",
    "Nous faisons le même type de fonction pour les deux autres catégories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d686c2-7b94-4bfd-a882-52776e312ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# récupération des carctéristiques  des ordinateurs de type g-series d'une page web donnée.\n",
    "def base_inter_gseries ( url_page):\n",
    "  elementss_ref= url(url_page)   # renvoie la liste l'url des machines sur url_page et les codes de commandes associés\n",
    "  data1= pd.DataFrame()  # initialisation du dataframe qui comportera  les machine d'une pageweb ainsi que leurs caractéristiques\n",
    "  for i in range (len(elementss_ref)):\n",
    "    data_inter= caract_g_series(elementss_ref[i])  # dataframe d'un ordi avec ses caractéristiques\n",
    "    data1= pd.concat([data1, data_inter],axis=1,sort=True) \n",
    "  return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459a943-1432-4845-a606-06fdb44584f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des carctéristiques  des ordinateurs de type aliewner  d'une page web donnée.\n",
    "def base_inter_aliewner ( url_page):\n",
    "  elementss_ref= url(url_page)   # renvoie la liste l'url des machines sur url_page et les codes de commandes associés\n",
    "  data1= pd.DataFrame()   # initialisation du dataframe qui comportera  les machine d'une pageweb ainsi que leurs caractéristiques\n",
    "  for i in range (len(elementss_ref)):\n",
    "    data_inter= caract_aliewener(elementss_ref[i])  # liste des caractéristiques de chaque ordimateur\n",
    "    data1= pd.concat([data1, data_inter],axis=1,sort=True)   \n",
    "  return data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7019a06-67a6-4367-b193-f061ee5191a5",
   "metadata": {},
   "source": [
    "Après avoir contitué nos bases intermédiaires( pour une seule page web), il est question maintenant les concatener pour avoir notre base finale selon les catégories. Pour le faire, nous avons besoin d'une fonction qui pour un type de machine, va sur toutes les pages qui lui sont associées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3d71e-4bf2-4664-8737-467315e95e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des caractéristiques des ordinateurs de type inspiron ou xps\n",
    "def base_finale_insp_xps (url_page):\n",
    "  base= pd.DataFrame()  # initialisation du dataframe\n",
    "  for i  in range (1,int(url_page[1])+1) :\n",
    "    url_pagefille1= url_page[0] + \"?page=\" + str(i)  # ième page web de la catégorie dont l'url est \"url_page\"\n",
    "    inter= base_inter_insp_xps(url_pagefille1,url_page[2],url_page[3])   \n",
    "    base = pd.concat([base,inter], axis=1)\n",
    "  return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed683762-ec36-4619-93e4-7aad762fdf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des caractéristiques des ordinateurs de type g_series\n",
    "def base_finale_gseries (url_page):\n",
    "  base= pd.DataFrame()  # initialisation du dataframe\n",
    "  for i  in range (1,int(url_page[1])+1) :\n",
    "    url_pagefille1= url_page[0] + \"?page=\" + str(i)  # ième page web de la catégorie dont l'url est \"url_page\"\n",
    "    inter= base_inter_gseries(url_pagefille1)   \n",
    "    base = pd.concat([base,inter], axis=1)\n",
    "  return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcc023-5efc-435d-8e47-9afc168be7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des caractéristiques des ordinateurs de type aliewner\n",
    "def base_finale_aliewnere (url_page):\n",
    "  base= pd.DataFrame()  # initialisation du dataframe\n",
    "  for i  in range (1,int(url_page[1])+1) :\n",
    "    url_pagefille1= url_page[0] + \"?page=\" + str(i)  # ième page web de la catégorie dont l'url est \"url_page\"\n",
    "    inter= base_inter_aliewnere(url_pagefille1)   # \n",
    "    base = pd.concat([base,inter], axis=1)\n",
    "  return base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b57a2-5ae6-4098-9db2-326218094e65",
   "metadata": {},
   "source": [
    "Les fonction ci-dessus , prennnent en paramètre, une liste dont le premier élément est l'url de la première page d'une catégorie la deuxième est le nombre de pages sur lesquelle elle tient et renvoie la base constituée des numéros de commande de toutes les machines d'une catégorie ainsi que leurs caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b49bc-53b7-47c7-8a64-e1053e3d49f5",
   "metadata": {},
   "source": [
    " 4. **Application de nos fonction et récupération des bases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161ae40-4c84-4ede-bd6b-d49629a5dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constitution de la base des caractéristiques pour les catégories inspirons et xps\n",
    "data_inpiron_xps = pd.dataFrame()\n",
    "for i  in range (2) :\n",
    "    data_inter = base_finale_insp_xps(list_ref_categ[i])\n",
    "    data_inpiron_xps = data_inpiron_xps.append(data_inter)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5f29b3-7275-46d5-b72a-30225983a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constitution de la base des caractéristiques pour les catégories g_series\n",
    "data_g_series= base_finale_gseries(list_ref_categ[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470eac1-afc3-43a9-b245-03139ba33712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constitution de la base des caractéristiques pour les catégories aliewner\n",
    "data_aliewner = base_finale_aliewnere(list_ref_categ[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c563d-5eb6-440d-ad8c-3f24f9b2ea87",
   "metadata": {},
   "source": [
    "Pour la suite des analyse, nous enrégistrons ces caractéristiques dans des fichiers csv car le scraping prend environ 1h30 et nous ne pourrons pas exécuter le code à chaque fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0318a5-a920-4157-91ce-cae737ed018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inpiron_xps.to_csv('data_inpiron_xps.csv')\n",
    "data_g_series.to_csv('data_g_series.csv')\n",
    "data_aliewner.to_csv('data_aliewner.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
