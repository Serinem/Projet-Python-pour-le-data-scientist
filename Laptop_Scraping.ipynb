{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b955bf0-6807-4f66-9085-d5951351f3f1",
   "metadata": {},
   "source": [
    "# PROJET DE PYTHON\n",
    "# THEME : ANALYSE DE LA SATISFACTION DES UTILISATEURS DE LAPTOPS : CAS DES LAPTOPS DELL\n",
    "# Auteurs:\n",
    "* MECHIDE SERINE\n",
    "* METHAFE KUITE SORELLE LOVELINE\n",
    "* NKAMENI DANIEL\n",
    "* POUANI EMAPI HORNELLA JOÊLLE\n",
    "\n",
    "# SCRAPING DES DONNEES SUR LES LAPTOPS DELL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1212da4-ae8e-4f20-ae6f-8914a7b04046",
   "metadata": {},
   "source": [
    "Le but de cette partie est de scraper tous les informations (caractéristiques et commentaires) sur les machines proposées sur le site officiel de Dell France au 10 Dcembre 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d2bdf3-9bf2-4a10-a521-fca5e761350b",
   "metadata": {},
   "source": [
    "## SCRAPING DES COMMENTAIRES SUR LES LAPTOPS DELL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3e910-2a3a-4771-b785-b5f465fdcd06",
   "metadata": {},
   "source": [
    "Pour scraper les commentaires de chaque machine, il incombe d'ouvrir individuellement les pages de chacune d'elles. Les liens vers la page détaillée de chaque machine sont obtenu à l'aide d'un premier craping. Les pages détaillées des machines sont dynamiques et chargées avec Java Script. Les méthodes de scraping basic vu en cours ne suffiront donc pas dans ce cas. Nous utilisons donc dans la suite le package Selenium adapt au scraping des pages dynamique.\n",
    "\n",
    "Le pilote Web utilisé est Chrome. l'étape suivante est donc de configurer ce pilote. On définit également un temps d'attente necessaire au chargement de chaque page. Nous avons défini un temps d'attente de 5 secondes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92b9765-85b3-41e4-836c-08125679f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des packages\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "import requests  \n",
    "import re\n",
    "import time  \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from joblib import Parallel, delayed \n",
    "import itertools\n",
    "import urllib\n",
    "import bs4\n",
    "import pandas as pd\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b112d-8f7c-48ed-a8c4-3e5f6c2f681b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Installer chrome driver\n",
    "driver_path = 'C:/Users/nkame/anaconda3/chromedriver.exe'\n",
    "opt = webdriver.ChromeOptions()\n",
    "opt.add_experimental_option('w3c', False)\n",
    "driver = webdriver.Chrome(executable_path=driver_path,options=opt)\n",
    "\n",
    "\n",
    "##Initialiser le driver de Selenium\n",
    "def setupDriver(url, waiting_time = 5):\n",
    "\tdriver = webdriver.Chrome(options=opt)\n",
    "\tdriver.get(url)\n",
    "\ttime.sleep(waiting_time) \n",
    "\treturn driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635be3e-0a42-4081-8e82-520131f14710",
   "metadata": {},
   "source": [
    "La page de chaque machine affiche les premiers commentaires de cette macine en bas de page. Pour les machines dontles commentaires ne tiennent pas sur une page, il faut retrouver l'url de la page suivante et l'utiliser pour acceder aux commentaires suivants. \n",
    "\n",
    "Attention: Pour la première page de commentaires comme les page suivante, il est important d'entrer exactement l'url de la section commentaire sinon l'extraction n'aura pas lieu.\n",
    "\n",
    "Le code suivant permet de charger chaque page detaillée et recupérer le lien de la page des commentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c61f3-441c-4b2e-a7f9-4879a7259d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Charger la page de chaque machine\n",
    "def getJSpage(url):\n",
    "\tdriver = setupDriver(url)\n",
    "\thtml = driver.page_source\n",
    "\tdriver.close()\n",
    "\treturn BeautifulSoup(html, features=\"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb310a-be35-4c1d-ac39-bf88764c0e24",
   "metadata": {},
   "source": [
    "Une fois la page des commentaires ouverte, il incombe de déterminer la classe qui sera utilisée pour l'extraction. l'inspection de la page revèle qu'il s'agit d'un objet \"div\" de classe \"bv-content-summary-body-text\". \n",
    "\n",
    "Dans cette classe, sont présents les commentaires des clients ainsi que ceux de Dell. Nous allons par la suite crire une fonction qui suprime les commentaires de Dell.\n",
    "\n",
    "Rappelonsque les commentaires de certaines machines peuvent se retrouver sur plus d'une page et doivent être pris en compte. La fonction \"getNextPage\" dans le code suivant permet donc de recuperer l'url de la page suivante dans la page actuelle afin de pouvoir s'y rendre une fois le scraping sur la page actuelle terminé.\n",
    "\n",
    "La fonction de recupération des commentaires d'une machine est conçue de façon à fonctionner de façon totalement autonome et adaptative en fonction du nombre de pages de commentaires disponibles pour cette machine. En effet, la fonction recupère les commentaires sur une page, vérifie s'il y a une page suivante et si oui passe à celle-ci et recupère les commentaires aussi. Elle s'arrête quand toutes les pages de commentaires ont t visit et scrapé. La structure du site de Dell est assez bizarre mais nous avons réussi à pouvoir implementer cette fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b3c37-6eb0-45ac-98f7-6ae418146049",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Recuperer l'ensemble des commentaires\n",
    "\n",
    "# Fonction qui recupère les commentaires d'une page\n",
    "def getPageComments(soupage):\n",
    "    cpage = soupage.findAll('div', {'class': 'bv-content-summary-body-text'})\n",
    "    pagecom = [x.text.strip() for x in cpage]\n",
    "    return pagecom\n",
    "\n",
    "# Fonction qui recupère l'URL de la prochaine page de commentaires\n",
    "def getNextPage(soupage):\n",
    "    nextPage = soupage.findAll('a', {'class': 'bv-content-btn bv-content-btn-pages bv-content-btn-pages-last bv-focusable bv-content-btn-pages-active'})\n",
    "    return nextPage\n",
    "\n",
    "# Fonction de recupration des commentaires pour une machine\n",
    "def getcomments(NCom,Machinelink):\n",
    "    PageM = getJSpage(Machinelink)\n",
    "    Pagecom = []\n",
    "    Pagecom = Pagecom + getPageComments(PageM)\n",
    "    nextPage = getNextPage(PageM)\n",
    "    \n",
    "    # Vérification de l'existence d'une page suivnate\n",
    "    while(len(nextPage) != 0):\n",
    "        PageM2 = getJSpage(nextPage[0][\"href\"])\n",
    "        Pagecom = Pagecom + getPageComments(PageM2)\n",
    "        nextPage = getNextPage(PageM2)\n",
    "    \n",
    "    # Création des indices (numéro de commande unique pour chaque machine) et de la fameuse base de données\n",
    "    MIndex = [NCom]*len(Pagecom)\n",
    "    comments = pd.DataFrame()\n",
    "    comments[\"Comments\"] = Pagecom\n",
    "    comments.index = MIndex\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b5d53-2f49-4a97-9061-139f190b357b",
   "metadata": {},
   "source": [
    "Une fois les commentaire récupérés, il faut supprimier les commentaires de Dell. Le souci est que tous les commentaires (clients et Dell) sont dans les mêmes balises et classes. Il est clair qu'utiliser les balises ou les classes pour les séparer allait être très compliqué. Nous avons donc analysé les commentaires de Dell et nous avons remarqu qu'ils avaient tous une signature éléctronique contenant \"@Dell\" ou \"@ Dell\".\n",
    "\n",
    "Ceci nous a données l'idée de scraper tous les commentaires et ensuite de supprimer tous ceux contenant @Dell ou @ Dell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01456f-3732-4ae6-ac19-7a8b4edc4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des commentaires de DELL\n",
    "def DelDellComments(Comments):\n",
    "    CleanComments = Comments[[(('@Dell' not in str(X))&('@ Dell' not in str(X))) for X in Comments[\"Comments\"]]]\n",
    "    return CleanComments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b4141-34f6-4fbc-bd74-f7e3e4f3aa17",
   "metadata": {},
   "source": [
    "Avec toutes ces fonctions, nous pouvons à prsent écrire la fonction de récupérations de tous les commentaires des machines disponibles sur le site de Dell. Cette fonction prend en entrée un Data Frame conténant les codes de commande (identificateurs unique de chaque machine) et les url de toutes les machines et renvoie un Data Frame de commenatires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9687d8c-3d4c-4586-8f7b-1e0a0fc84a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la fonction de recupération de tous les commentaires\n",
    "def getAllComments(Machines):\n",
    "    AllComments = pd.DataFrame()\n",
    "    for i in range(len(Machines)):\n",
    "        Com = getcomments(Machines.iloc[i,0],Machines.iloc[i,1])\n",
    "        AllComments = pd.concat([AllComments,Com],axis=0)\n",
    "    AllComments = DelDellComments(AllComments)\n",
    "    return AllComments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e8b67-1beb-420a-a42e-9b90f1bc12ac",
   "metadata": {},
   "source": [
    "Le bloc de code suivant permet de récupérer les url de toutes les machines à l'aide d'un scraping classique sur le site de de Dell. Ceci permettra de pouvoir construire le Data Frame néccessaire à la récupération des commentaires comme décrit plus haut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc832f64-8df3-4417-a2c6-12a058e0bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catégories de machines et nombre de pages sur le site de Dell\n",
    "\n",
    "list_ref_categ = [[\"https://www.dell.com/fr-fr/shop/ordinateurs-portables-dell/sr/laptops/inspiron-laptops\",6] , [\"https://www.dell.com/fr-fr/shop/ordinateurs-portables-dell/sr/laptops/xps-laptops\",2], [\"https://www.dell.com/fr-fr/shop/ordinateurs-portables-dell/sr/laptops/g-series\",2] , [\"https://www.dell.com/fr-fr/shop/ordinateurs-portables-dell/sr/laptops/alienware-laptops\",3 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06b957-2e46-45c3-80c7-f26aa9e71e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de récupérateion des codes des dmachines et les url de toutes les machines\n",
    "\n",
    "def url (url):\n",
    "  el=[]\n",
    "  codess=[]\n",
    "  for i  in range (1,int(url[1])+1) :\n",
    "    urlel= url[0] + \"?page=\" + str(i) \n",
    "    req = request.Request(urlel,headers={'User-Agent': 'Mozilla/5.0'})    \n",
    "    html = request.urlopen(req).read()\n",
    "    pagefille= bs4.BeautifulSoup(html,\"lxml\")\n",
    "    elementss = pagefille.find_all('div', class_ = \"ps-image ps-product-image\")  # elementss est une liste dont chaque élément contient une balise de type \"a\" contenant l'url de la machine considérée\n",
    "    code = pagefille.find_all('div', class_ = \"ps-compare\") # code est une liste dont chaque élément contient une balise de type \"label\" contenant le code de commande d'une machine\n",
    "    elementss_ref= [element.a['href'] for element in elementss]\n",
    "    el = el + elementss_ref\n",
    "    codes=[cod.label['oc'] for cod in code ]\n",
    "    codess = codess + codes\n",
    "  return el, codess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a4527-2cb5-4f1f-95ad-df270aca054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération proprement dite des codes machines et url\n",
    "\n",
    "elements=[]\n",
    "codes=[]\n",
    "\n",
    "for i in range(len(list_ref_categ)):\n",
    "  el,codess = url(list_ref_categ[i])\n",
    "  elements = elements + el\n",
    "  codes = codes + codess\n",
    "\n",
    "useful_urls = [\"https:\" + str(raw_url) + \"#ratings_section\" for raw_url in elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ad036-ecf7-4ae7-9fc9-cea06dfbc425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du Data Frame contenant toutes les machines\n",
    "\n",
    "All_machines = pd.DataFrame({'code_commande': codes, 'url_ordi':useful_urls}).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb6abf-a4fa-4f9c-b8aa-bd80bd7f70ac",
   "metadata": {},
   "source": [
    "A ce stade, nous pouvons enfin récupérer les commentaires de toutes les machines sur le site de Dell France..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b392e8-775f-4af1-9b5b-882d5c5b5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webscraping des commentaires hahaha\n",
    "# NB: La fonction utilisée ici supprime aussi les commentaire de Dell et garde juste les commentaires clients\n",
    "\n",
    "All_comments = getAllComments(All_machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157458fb-29d7-470d-8cb0-a1daa5a69088",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9bdcce-6e01-4989-a3bf-8b6d00255935",
   "metadata": {},
   "source": [
    "Pour la suite des analyse, nous enrégistrons ces commentaires dans un fichier Excel car le scraping prend environ 1h30 et nous ne pourrons pas exécuter le code à chaque fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e516f-99bc-4927-b4df-9077611c55fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_comments.to_excel(\"C:/Users/nkame/Desktop/ND/COURS ET TD/PYTHON/PROJET/Commentaires.xlsx\", header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a7c50-73ba-42b1-9716-788ff8073984",
   "metadata": {},
   "source": [
    "## SCRAPING DES cra$ SUR LES LAPTOPS DELL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
